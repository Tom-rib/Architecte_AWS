# AWS Glue Crawlers - Guide Complet ğŸ”

Tout sur les crawlers Glue pour dÃ©tecter et cataloguer vos donnÃ©es.

---

## TABLE DES MATIÃˆRES

1. [Qu'est-ce qu'un Crawler ?](#qu-est-ce-quun-crawler-)
2. [Fonctionnement](#fonctionnement)
3. [Configuration](#configuration)
4. [Sources SupportÃ©es](#sources-supportÃ©es)
5. [SchÃ©ma Detection](#schÃ©ma-detection)
6. [Partitions](#partitions)
7. [Scheduling](#scheduling)
8. [Best Practices](#best-practices)

---

## Qu'est-ce qu'un Crawler ?

Service qui :
- Scanne vos donnÃ©es
- DÃ©tecte schÃ©mas automatiquement
- CrÃ©e/met Ã  jour table Catalog
- Aucun code Ã  Ã©crire

```
Crawler
â”‚
â”œâ”€ EntrÃ©e: S3 path (ou RDS, Redshift, etc)
â”œâ”€ OpÃ©ration: Lire sample + dÃ©tecter
â”œâ”€ Sortie: Table dans Glue Catalog
â””â”€ Trigger: Schedule ou manual
```

---

## Fonctionnement

### Ã‰tapes d'ExÃ©cution

```
1. Lancer Crawler
2. Connecter Ã  source (S3, RDS, etc)
3. Scanner fichiers/tables
4. Lire sample rows
5. InfÃ©rer types:
   â”œâ”€ "123" â†’ integer
   â”œâ”€ "hello" â†’ string
   â”œâ”€ "2024-01-01" â†’ date
   â””â”€ "45.67" â†’ double
6. CrÃ©er schÃ©ma
7. CrÃ©er/mettre Ã  jour table Catalog
8. Enregistrer partitions (si existent)
```

### DÃ©tection Smart

```
CSV File:
id,name,created_at
1,Alice,2024-01-01
2,Bob,2024-01-02
3,Charlie,2024-01-03

Crawler detects:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column     â”‚ Type â”‚ Nullable  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id         â”‚ int  â”‚ false     â”‚
â”‚ name       â”‚ str  â”‚ false     â”‚
â”‚ created_at â”‚ date â”‚ false     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration

### CrÃ©er un Crawler (Console)

```
1. AWS Glue > Crawlers
2. "Create crawler"
3. Remplir:
   â”œâ”€ Name: customers-crawler
   â”œâ”€ Source:
   â”‚  â”œâ”€ Type: S3
   â”‚  â”œâ”€ Path: s3://mybucket/customers/
   â”‚  â””â”€ Include patterns: *.csv
   â”œâ”€ Output:
   â”‚  â”œâ”€ Database: default
   â”‚  â””â”€ Table prefix: raw_
   â”œâ”€ Schedule: Daily 00:00
   â””â”€ "Create"
```

### Via CLI

```bash
aws glue create-crawler \
  --name customers-crawler \
  --database-name default \
  --description "Crawl customer data" \
  --targets S3Targets=[{Path=s3://mybucket/customers/}] \
  --role arn:aws:iam::ACCOUNT_ID:role/GlueServiceRole \
  --schedule-expression "cron(0 0 * * ? *)" \
  --region eu-west-3
```

---

## Sources SupportÃ©es

```
Crawler peut lire:
â”œâ”€ S3 (fichiers)
â”œâ”€ RDS (tables)
â”œâ”€ Redshift (tables)
â”œâ”€ DynamoDB (tables)
â”œâ”€ JDBC (database)
â”œâ”€ MongoDB (collections)
â””â”€ Kafka (topics)
```

---

## SchÃ©ma Detection

### Type Inference

```
Glue essaie de dÃ©tecter le type correct

Exemples:
"2024-01-01" â†’ timestamp (avec context)
"2024" â†’ int (sans context)
"45.67" â†’ double
"true" / "false" â†’ boolean
"123" â†’ int

ProblÃ¨mes:
"NA" â†’ string (not double)
"" â†’ string (empty)
NULL â†’ inferred from other rows
```

### Partition Detection

```
Crawler dÃ©tecte partitions auto

Exemple S3:
s3://bucket/data/
â”œâ”€ year=2024/month=01/customers.csv
â”œâ”€ year=2024/month=02/customers.csv
â””â”€ year=2025/month=01/customers.csv

Crawler crÃ©e:
Table: data
Partitions: year (int), month (int)

Avantage: RequÃªte annÃ©e=2024 scanne seulement 2024
```

---

## Partitions

### Partition Keys

```
DÃ©finir partitions pour optimiser requÃªtes:

Avant:
s3://bucket/customers/ (1GB â†’ 5 sec query)

AprÃ¨s (partitionnÃ©):
s3://bucket/customers/year=2024/month=01/ (100MB â†’ 0.5 sec)

Crawler peut auto-dÃ©tecter structure S3:
year=2024/month=01/ â†’ partitions year, month
```

---

## Scheduling

### Options

```
On-demand:
â”œâ”€ Crawler runs manuellement
â”œâ”€ Quand: aws glue start-crawler
â””â”€ Use: Testing, debug

Scheduled:
â”œâ”€ Crawler runs automatiquement
â”œâ”€ Options:
â”‚  â”œâ”€ Hourly: cron(0 * * * ? *)
â”‚  â”œâ”€ Daily: cron(0 0 * * ? *)
â”‚  â”œâ”€ Weekly: cron(0 0 ? * MON *)
â”‚  â””â”€ Custom: cron(15 12 * * ? *)
â””â”€ Use: Production daily crawls

CloudWatch Events:
â”œâ”€ Trigger sur Ã©vÃ©nement
â”œâ”€ Ex: S3 object created
â””â”€ Use: Real-time updates
```

---

## Best Practices

### 1. Naming

```
âŒ crawler1, c1, data_crawler_v2

âœ… customers-raw-crawler
âœ… orders-raw-crawler
âœ… logs-archive-crawler
```

### 2. Database Organization

```
âŒ Tout dans "default" database

âœ… Separate databases:
â”œâ”€ database: raw_data
â”‚  â”œâ”€ table: customers
â”‚  â””â”€ table: orders
â””â”€ database: clean_data
   â”œâ”€ table: customers_clean
   â””â”€ table: orders_clean
```

### 3. Include/Exclude Patterns

```
Include patterns:
â”œâ”€ *.csv (tous CSV)
â”œâ”€ *.parquet (tous Parquet)
â””â”€ data_*.json (data files seulement)

Exclude patterns:
â”œâ”€ _temp/* (temp files)
â”œâ”€ .logs/* (logs)
â””â”€ *.tmp (temporary)
```

### 4. Schedule Optimization

```
âŒ Crawler toutes les heures (si data changes daily)
â”œâ”€ CoÃ»teux
â”œâ”€ Inutile

âœ… Crawler une fois par jour (aprÃ¨s bulk load)
â”œâ”€ Efficace
â”œâ”€ Suffit pour metadata
```

---

## DÃ©pannage

### Crawler runs mais aucune table crÃ©Ã©e

```
Causes:
1. Source vide (no files)
2. Wrong path (typo)
3. Include patterns trop restrictif
4. IAM permissions manquantes

Solutions:
1. VÃ©rifier chemin S3
2. VÃ©rifier fichiers existent
3. Ã‰largir patterns
4. Ajouter S3 read permissions
```

### Types dÃ©tectÃ©s incorrectement

```
Cause: Sample rows ambigus

Solution:
1. VÃ©rifier donnÃ©es source
2. Augmenter sample size (crawler settings)
3. Editer schÃ©ma manuellement aprÃ¨s crawl
4. Utiliser Glue jobs pour fix
```

---

**CoÃ»t**: 1M DPU-seconds/mois GRATUIT (puis $0.44/DPU-hour)
