# AWS Glue Jobs - Guide AvancÃ© ğŸš€

Tout sur la crÃ©ation et exÃ©cution de jobs de transformation.

---

## CrÃ©er un Job Glue

### Minimal Job

```python
import sys
from awsglue.transforms import *
from awsglue.context import GlueContext
from pyspark.context import SparkContext

sc = SparkContext()
glueContext = GlueContext(sc)

# Lire depuis S3
df = glueContext.create_dynamic_frame.from_catalog(
    database="default",
    table_name="customers"
)

# Ã‰crire vers S3
glueContext.write_dynamic_frame.from_options(
    frame=df,
    connection_type="s3",
    connection_options={"path": "s3://output/customers/"},
    format="parquet"
)
```

### Avec Transformation

```python
from awsglue.transforms import Filter, ApplyMapping

# Charger
customers = glueContext.create_dynamic_frame.from_catalog(
    database="default",
    table_name="customers"
)

# Filtrer (supprimer nulls)
clean_customers = Filter.apply(
    customers,
    lambda row: row["email"] is not None
)

# Mapper colonnes (rename/reorder)
mapped = ApplyMapping.apply(
    frame=clean_customers,
    mappings=[
        ("id", "bigint", "customer_id", "bigint"),
        ("name", "string", "full_name", "string"),
        ("email", "string", "email_address", "string"),
        ("created_at", "timestamp", "created_date", "string"),
    ]
)

# Sauvegarder
glueContext.write_dynamic_frame.from_options(
    frame=mapped,
    connection_type="s3",
    connection_options={"path": "s3://output/customers_clean/"},
    format="parquet"
)
```

---

## Job Parameters

```
Command-line args:

--database: source database
--table: source table
--output_path: destination S3

Usage:
aws glue start-job-run \
  --job-name customers-transform \
  --arguments '--database=default,--table=customers,--output_path=s3://out/'
```

---

## Job Monitoring

```
CloudWatch Logs:
â”œâ”€ /aws-glue/jobs/customers-transform
â”œâ”€ stdout (print statements)
â””â”€ stderr (errors)

Metrics:
â”œâ”€ Execution time
â”œâ”€ DPU hours consumed
â””â”€ Success/failure
```

---

## DPU Allocation

```
Worker Types:

G.1X (Default):
â”œâ”€ 4 vCPU + 16 GB RAM
â”œâ”€ Cost: $0.44/hour
â””â”€ Good for: most jobs

G.2X (2x power):
â”œâ”€ 8 vCPU + 32 GB RAM
â”œâ”€ Cost: $0.88/hour
â””â”€ Good for: heavy compute

G.025X (lightweight):
â”œâ”€ 0.25 vCPU + 1 GB RAM
â”œâ”€ Cost: $0.055/hour
â””â”€ Good for: Python shell
```

---

## CoÃ»ts

```
Job de 10 DPUs pendant 2 heures:
10 DPUs Ã— 2 hours Ã— $0.44/DPU-hour = $8.80

Optimizations:
â”œâ”€ Use right-sizing (avoid over-allocation)
â”œâ”€ Cache data between runs
â”œâ”€ Use efficient formats (Parquet)
â””â”€ Partition source data
```

---

## Erreurs Courantes

```
âŒ OOM (Out of Memory)
â””â”€ Augmenter DPUs ou optimiser code

âŒ Timeout
â””â”€ Augmenter job timeout (par dÃ©faut 2880 min)

âŒ Schema mismatch
â””â”€ VÃ©rifier crawler a dÃ©tectÃ© correct schÃ©ma
```

---

**SUITE**: Voir 07-PySpark-Glue.md pour code avancÃ©
