# AWS Glue - Guide Complet & RÃ©fÃ©rence ğŸ”§

Service AWS pour dÃ©couvrir, prÃ©parer et transformer les donnÃ©es Ã  grande Ã©chelle.

---

## TABLE DES MATIÃˆRES

1. [Qu'est-ce que AWS Glue ?](#qu-est-ce-que-aws-glue-)
2. [Composants Principaux](#composants-principaux)
3. [Glue Catalog](#glue-catalog)
4. [Crawlers](#crawlers)
5. [Jobs](#jobs)
6. [Architecture](#architecture)
7. [Pricing](#pricing)
8. [Best Practices](#best-practices)

---

## Qu'est-ce que AWS Glue ?

Service **ETL (Extract, Transform, Load)** entiÃ¨rement gÃ©rÃ© qui facilite :

```
Extract (Extraire)
â”œâ”€ Lire donnÃ©es depuis S3, RDS, Redshift, DynamoDB
â””â”€ DÃ©tecter schÃ©mas automatiquement

Transform (Transformer)
â”œâ”€ Nettoyer donnÃ©es (supprimer nulls, valeurs invalides)
â”œâ”€ Convertir formats (CSV â†’ Parquet)
â”œâ”€ Joindre tables
â””â”€ Enrichir avec lookups

Load (Charger)
â”œâ”€ Ã‰crire dans S3
â”œâ”€ Envoyer vers Redshift
â”œâ”€ Ou autre destination
â””â”€ PartitionnÃ© et optimisÃ©
```

### Comparaison avec alternatives

| Outil | Glue | Lambda | EC2 | DataPipeline |
|---|---|---|---|---|
| **Type** | ETL Managed | Serverless | On-premise | Orchestration |
| **Scaling** | Auto | Auto | Manual | Auto |
| **CoÃ»t** | Pay per DPU | Pay per exec | Pay per hour | Moins cher |
| **IdÃ©al pour** | Gros volumes | Petit jobs | Full control | Workflow |
| **Langage** | PySpark, Scala | Python, Node | Tous | Shell script |

---

## Composants Principaux

### 1. CRAWLER (Explorateur)

**RÃ´le** : Parcourir S3 et dÃ©tecter les schÃ©mas automatiquement

```
Crawler
â”‚
â”œâ”€ Lit fichiers S3
â”œâ”€ DÃ©tecte colonnes
â”œâ”€ DÃ©tecte types (int, string, date)
â”œâ”€ CrÃ©e table dans Catalog
â””â”€ Planifiable (hourly, daily, on-demand)
```

**Exemple** :
```
Input: s3://mybucket/customers/
â”œâ”€ customers.csv
â”‚  â””â”€ id, name, email, age
â””â”€ customers_new.csv
   â””â”€ id, name, email, age, phone

Crawler crÃ©e/met Ã  jour:
Table: customers
SchÃ©ma: id (int), name (string), email (string), age (int), phone (string)
```

**CoÃ»t** : Gratuit (1M requÃªtes/mois incluses)

---

### 2. CATALOG (MÃ©tadonnÃ©es)

**RÃ´le** : Stocker mÃ©tadonnÃ©es (schÃ©mas, types, partitions)

```
Glue Catalog
â”‚
â”œâ”€ Database (container)
â”‚  â””â”€ Tables (schÃ©mas)
â”‚     â”œâ”€ Colonnes
â”‚     â”œâ”€ Types
â”‚     â”œâ”€ Partitions
â”‚     â”œâ”€ Location S3
â”‚     â””â”€ Format
â”‚
â””â”€ Accessible par:
   â”œâ”€ Glue Jobs
   â”œâ”€ Athena (requÃªtes SQL)
   â”œâ”€ Redshift Spectrum
   â””â”€ EMR
```

**Avantages** :
- Schema once, use everywhere
- Pas de duplication de metadata
- CentralisÃ© et versionnÃ©
- Searchable

---

### 3. JOB (Transformation)

**RÃ´le** : ExÃ©cuter code PySpark/Scala pour transformer donnÃ©es

```
Job Glue
â”‚
â”œâ”€ Lit depuis Catalog
â”œâ”€ ExÃ©cute code PySpark
â”œâ”€ Transforme donnÃ©es
â””â”€ Ã‰crit vers S3/Redshift/etc

Architecture:
Job â†’ Spark cluster (auto-scaled)
     â†’ DPU allocation (1-100 DPUs)
     â†’ ParallÃ©lisation auto
     â†’ CoÃ»ts basÃ©s sur temps exec
```

**Langage** : PySpark (Python) ou Scala

**Execution** :
- On-demand (manual trigger)
- Scheduled (cron)
- Event-based (S3 upload)

---

### 4. TRIGGER (Orchestration)

**RÃ´le** : Automatiser exÃ©cution de jobs/crawlers

```
Trigger
â”‚
â”œâ”€ Types:
â”‚  â”œâ”€ Crawler complete (aprÃ¨s crawler fini)
â”‚  â”œâ”€ Scheduled (cron: daily, hourly, etc)
â”‚  â”œâ”€ On-demand (manual)
â”‚  â””â”€ CloudWatch Events
â”‚
â””â”€ Actions:
   â”œâ”€ Lancer crawler
   â”œâ”€ Lancer job
   â””â”€ Notification SNS
```

---

## Glue Catalog

### Database

Container pour organiser tables

```
database: sales_data
â”œâ”€ Table: customers
â”œâ”€ Table: orders
â””â”€ Table: products

database: logs
â”œâ”€ Table: app_logs
â””â”€ Table: error_logs
```

### Table

SchÃ©ma + mÃ©tadonnÃ©es

```
Table: customers
â”œâ”€ Location: s3://mybucket/customers/
â”œâ”€ Format: parquet
â”œâ”€ Partitions: year, month
â”‚
â”œâ”€ Colonnes:
â”‚  â”œâ”€ id (bigint) - primary
â”‚  â”œâ”€ name (string)
â”‚  â”œâ”€ email (string)
â”‚  â”œâ”€ phone (string)
â”‚  â”œâ”€ created_at (timestamp)
â”‚  â””â”€ year (int) - partition
â”‚
â””â”€ Properties:
   â”œâ”€ Classification: parquet
   â”œâ”€ Compression: snappy
   â””â”€ Stored as Parquet
```

### Partitions

Optimiser requÃªtes par division donnÃ©es

```
Sans partition:
s3://mybucket/customers/
â”œâ”€ customer_1.parquet
â”œâ”€ customer_2.parquet
â””â”€ ... (tout dans un endroit)

Avec partition (year/month):
s3://mybucket/customers/
â”œâ”€ year=2024/month=01/
â”‚  â”œâ”€ part_01.parquet
â”‚  â””â”€ part_02.parquet
â”œâ”€ year=2024/month=02/
â”‚  â””â”€ part_01.parquet
â””â”€ year=2025/month=01/
   â””â”€ part_01.parquet

Avantage: RequÃªte year=2024 ne lit que 2024 (10x plus rapide)
```

---

## Crawlers

### Fonctionnement

```
1. Lancer Crawler
   â”‚
2. Connecter Ã  S3
   â”‚
3. Scanner fichiers
   â”œâ”€ Lire sample rows
   â”œâ”€ DÃ©tecter types
   â”œâ”€ Grouper partitions
   â”‚
4. CrÃ©er/mettre Ã  jour table Catalog
   â”‚
5. Enregistrer metadonnÃ©es
   â”œâ”€ Schema
   â”œâ”€ Partitions
   â”œâ”€ Format
   â””â”€ Classification
```

### Configuration

```
Crawler Config:
â”œâ”€ Name: data-crawler
â”œâ”€ Source: S3 path (s3://mybucket/data/)
â”œâ”€ Include patterns: *.csv, *.parquet
â”œâ”€ Exclude patterns: _temp, .logs
â”œâ”€ Database output: default
â”œâ”€ Table name prefix: raw_
â”œâ”€ Partitions: year, month
â”œâ”€ Data format: auto-detect
â””â”€ Schedule: daily at 00:00
```

### CoÃ»ts

```
Free tier: 1M DPU-seconds/mois GRATUIT

Calculation:
Crawler run time = 5 minutes
Cost = (1 DPU Ã— 5 min / 60 min) = 0.083 DPU-hours
Monthly: 30 runs Ã— 0.083 = 2.5 DPU-hours = 1.1$ (aprÃ¨s free tier)
```

---

## Jobs

### Anatomy of a Glue Job

```python
import sys
from awsglue.transforms import *
from awsglue.context import GlueContext
from pyspark.context import SparkContext

# Context (Spark + Glue)
sc = SparkContext()
glueContext = GlueContext(sc)

# 1. EXTRACT - Read from Catalog
input_data = glueContext.create_dynamic_frame.from_catalog(
    database="default",
    table_name="customers"
)

# 2. TRANSFORM - Clean/Process
clean_data = Filter.apply(
    input_data,
    lambda row: row is not None and row["email"] is not None
)

# 3. LOAD - Write to S3
glueContext.write_dynamic_frame.from_options(
    frame=clean_data,
    connection_type="s3",
    connection_options={"path": "s3://output/customers_clean/"},
    format="parquet"
)
```

### Job Types

```
Standard Job
â”œâ”€ PySpark (Python + Spark)
â”œâ”€ Scala (Scala + Spark)
â””â”€ IdÃ©al pour ETL volumÃ©trique

Streaming Job
â”œâ”€ Kafka input
â”œâ”€ Real-time processing
â””â”€ IdÃ©al pour streaming data

Python Shell
â”œâ”€ Python sans Spark
â”œâ”€ Light-weight tasks
â””â”€ Moins cher
```

### DPU (Data Processing Units)

```
DPU = unitÃ© compute pour jobs

1 DPU = 4 vCPU + 16 GB RAM

Allocation:
â”œâ”€ G.1X: 1 DPU standard (dÃ©fault)
â”œâ”€ G.2X: 2 DPU (2x puissance)
â””â”€ G.025X: 0.25 DPU (workers)

Job config:
â”œâ”€ Worker type: G.1X (default)
â”œâ”€ Number of workers: 10 (10 DPUs)
â”œâ”€ Max concurrent runs: 1
â””â”€ Timeout: 2880 minutes (48h)

CoÃ»t:
10 DPUs Ã— 1 hour = $4.40 (0.44$/DPU-hour)
```

---

## Architecture

### Glue ETL Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Sources                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3     â”‚   RDS    â”‚ Redshift â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AWS Glue Crawler                 â”‚
â”‚ (Detect schema & create Catalog)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Glue Catalog                     â”‚
â”‚ (Store metadata)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AWS Glue Job (PySpark)           â”‚
â”‚ (Transform/Clean/Enrich)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3     â”‚ Redshift â”‚ Athena   â”‚
â”‚ (clean)  â”‚(warehouse)â”‚(queries) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pricing

### Free Tier

```
âœ… Crawlers: 1M DPU-seconds/mois
âœ… Catalog: Unlimited metadata
âœ… Triggers: Unlimited
âœ… Data Catalog: 100 tables free

Calculation:
1M DPU-seconds = ~278 DPU-hours
= ~278 crawler runs Ã— 1h each
```

### Payant

```
Jobs: $0.44 / DPU-hour (aprÃ¨s free tier)

Examples:
â”œâ”€ 10 DPU Ã— 2 hours = $8.80
â”œâ”€ 50 DPU Ã— 1 hour = $22
â””â”€ 100 DPU Ã— 30 min = $22

S3 Costs (separate):
â”œâ”€ Storage: $0.023 / GB / month
â”œâ”€ PUT/POST: $0.005 per 1000
â””â”€ GET: $0.0004 per 1000
```

---

## Best Practices

### 1. Partitions

```
âŒ BAD
s3://bucket/data/
â””â”€ 1GB file (slow queries)

âœ… GOOD
s3://bucket/data/
â”œâ”€ year=2024/month=01/
â”‚  â””â”€ part_001.parquet
â”œâ”€ year=2024/month=02/
â”‚  â””â”€ part_001.parquet
â””â”€ year=2025/month=01/
   â””â”€ part_001.parquet

Benefits:
- Faster queries (scan only needed partition)
- Better scalability
- Lower costs
```

### 2. Formats

```
âŒ CSV (bad for Glue)
â”œâ”€ Verbose
â”œâ”€ Schema lost
â””â”€ Slow parsing

âœ… Parquet (best)
â”œâ”€ Compressed
â”œâ”€ Schema preserved
â”œâ”€ Columnar (fast queries)
â””â”€ Compatible with everything
```

### 3. DPU Allocation

```
âŒ BAD
- Always use 100 DPU
- Oversized for small jobs
- Wasting money

âœ… GOOD
- Use right-sizing
- Monitor execution time
- Scale based on data volume
- Use G.025X for small jobs
```

### 4. Error Handling

```
âœ… Log errors
glueContext.getLogger().info("Processing...")

âœ… Add try-except
try:
    data = transform(input_data)
except Exception as e:
    glueContext.getLogger().error(f"Error: {e}")

âœ… Monitor in CloudWatch
See Job logs for debugging
```

---

**SUITE**: Voir 02-Crawlers-Concepts-Complets.md pour crawlers avancÃ©s
