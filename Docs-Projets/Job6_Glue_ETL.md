# Job 6 : AWS Glue - Pipeline ETL ðŸ”„

> Automatiser l'ingestion et la transformation de donnÃ©es

---

## ðŸŽ¯ Objectif

Automatiser l'ingestion et la transformation de donnÃ©es avec AWS Glue. Le pipeline extrait des donnÃ©es brutes d'un bucket S3, les nettoie, et gÃ©nÃ¨re des fichiers Parquet optimisÃ©s.

---

## ðŸ“¦ Ressources AWS UtilisÃ©es

| Service | RÃ´le |
|---------|------|
| AWS Glue | ETL serverless |
| S3 | Stockage input/output |
| Glue Crawler | DÃ©tection de schÃ©ma |
| Glue Data Catalog | MÃ©tadonnÃ©es |
| IAM | Permissions |

---

## ðŸ’° CoÃ»ts

| Service | Free Tier |
|---------|-----------|
| Glue Crawler | 1M runs gratuits |
| Glue ETL | ~$0.44/DPU-heure |

âš ï¸ **Attention** : Glue peut coÃ»ter si vous laissez des jobs tourner !

---

## ðŸ—ï¸ Architecture

```
S3 (CSV) â†’ Crawler â†’ Glue Catalog â†’ ETL Job â†’ S3 (Parquet)
```

---

# Ã‰tape 1 : PrÃ©parer les donnÃ©es dans S3

## CrÃ©er la structure S3

```
s3://mon-bucket-glue/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ customers.csv
â”œâ”€â”€ output/
â”‚   â””â”€â”€ (fichiers Parquet gÃ©nÃ©rÃ©s)
â””â”€â”€ scripts/
    â””â”€â”€ (scripts PySpark)
```

## Fichier de donnÃ©es : customers.csv

```csv
id,name,email,created_at
1,Alice Dupont,alice@example.com,2024-01-01
2,Bob Martin,bob@example.com,2024-01-05
3,Charlie Durand,charlie@example.com,2024-01-10
4,Diana Petit,,2024-01-15
5,Eve Bernard,eve@example.com,2024-01-20
6,,frank@example.com,2024-01-25
7,Grace Moreau,grace@example.com,2024-01-30
```

## ðŸ–¥ï¸ Dashboard

```
1. S3 â†’ Create bucket
   - Bucket name : mon-bucket-glue-VOTREPRENOM
   - Region : eu-west-3
   - Create bucket âœ“

2. CrÃ©er les dossiers :
   - Create folder â†’ input
   - Create folder â†’ output
   - Create folder â†’ scripts

3. Uploader le CSV :
   - input/ â†’ Upload â†’ customers.csv
```

## ðŸ’» CLI

```bash
# CrÃ©er le bucket
aws s3 mb s3://mon-bucket-glue-tom --region eu-west-3

# CrÃ©er le fichier CSV
cat > customers.csv << 'EOF'
id,name,email,created_at
1,Alice Dupont,alice@example.com,2024-01-01
2,Bob Martin,bob@example.com,2024-01-05
3,Charlie Durand,charlie@example.com,2024-01-10
4,Diana Petit,,2024-01-15
5,Eve Bernard,eve@example.com,2024-01-20
6,,frank@example.com,2024-01-25
7,Grace Moreau,grace@example.com,2024-01-30
EOF

# Uploader
aws s3 cp customers.csv s3://mon-bucket-glue-tom/input/
```

---

# Ã‰tape 2 : CrÃ©er le rÃ´le IAM pour Glue

## ðŸ–¥ï¸ Dashboard

```
1. IAM â†’ Roles â†’ Create role

2. Trusted entity : AWS service
   Use case : Glue

3. Next

4. Permissions :
   - â˜‘ AWSGlueServiceRole
   - â˜‘ AmazonS3FullAccess

5. Next

6. Role name : GlueServiceRole

7. Create role âœ“
```

## ðŸ’» CLI

```bash
# CrÃ©er le rÃ´le
aws iam create-role \
  --role-name GlueServiceRole \
  --assume-role-policy-document '{
    "Version": "2012-10-17",
    "Statement": [{
      "Effect": "Allow",
      "Principal": {"Service": "glue.amazonaws.com"},
      "Action": "sts:AssumeRole"
    }]
  }'

# Attacher les policies
aws iam attach-role-policy \
  --role-name GlueServiceRole \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

aws iam attach-role-policy \
  --role-name GlueServiceRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
```

---

# Ã‰tape 3 : CrÃ©er la base de donnÃ©es Glue

## ðŸ–¥ï¸ Dashboard

```
1. AWS Glue â†’ Databases â†’ Add database

2. Name : glue_demo_db

3. Create database âœ“
```

## ðŸ’» CLI

```bash
aws glue create-database \
  --database-input '{"Name": "glue_demo_db"}' \
  --region eu-west-3
```

---

# Ã‰tape 4 : CrÃ©er un Crawler

## ðŸ–¥ï¸ Dashboard

```
1. AWS Glue â†’ Crawlers â†’ Create crawler

2. Name : customers-crawler

3. Next

4. Data sources â†’ Add a data source :
   - Data source : S3
   - S3 path : s3://mon-bucket-glue-tom/input/
   - Add an S3 data source âœ“

5. Next

6. IAM role : GlueServiceRole

7. Next

8. Target database : glue_demo_db

9. Crawler schedule : On demand

10. Next â†’ Create crawler âœ“
```

## ðŸ’» CLI

```bash
aws glue create-crawler \
  --name customers-crawler \
  --role GlueServiceRole \
  --database-name glue_demo_db \
  --targets '{
    "S3Targets": [{
      "Path": "s3://mon-bucket-glue-tom/input/"
    }]
  }' \
  --region eu-west-3
```

---

# Ã‰tape 5 : ExÃ©cuter le Crawler

## ðŸ–¥ï¸ Dashboard

```
1. AWS Glue â†’ Crawlers â†’ customers-crawler

2. Run crawler âœ“

3. Attendez que le status soit "Ready" (2-3 minutes)

4. VÃ©rifiez la table crÃ©Ã©e :
   AWS Glue â†’ Tables â†’ input (ou customers)
```

## ðŸ’» CLI

```bash
# Lancer le crawler
aws glue start-crawler \
  --name customers-crawler \
  --region eu-west-3

# VÃ©rifier le status
aws glue get-crawler \
  --name customers-crawler \
  --query 'Crawler.State' \
  --region eu-west-3

# Voir la table crÃ©Ã©e
aws glue get-tables \
  --database-name glue_demo_db \
  --query 'TableList[*].Name' \
  --region eu-west-3
```

---

# Ã‰tape 6 : CrÃ©er le Job ETL

## Script PySpark : transform_customers.py

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame

# Initialisation
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Lire les donnÃ©es depuis le catalogue
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="glue_demo_db",
    table_name="input"
)

# Convertir en DataFrame pour les transformations
df = datasource.toDF()

# Afficher le schÃ©ma
print("SchÃ©ma original:")
df.printSchema()

# Transformation : Supprimer les lignes avec des valeurs NULL
df_cleaned = df.filter(
    (df["id"].isNotNull()) & 
    (df["email"].isNotNull())
)

# Afficher les stats
print(f"Lignes avant nettoyage: {df.count()}")
print(f"Lignes aprÃ¨s nettoyage: {df_cleaned.count()}")

# Reconvertir en DynamicFrame
cleaned_dynamic_frame = DynamicFrame.fromDF(df_cleaned, glueContext, "cleaned")

# Ã‰crire en Parquet dans S3
glueContext.write_dynamic_frame.from_options(
    frame=cleaned_dynamic_frame,
    connection_type="s3",
    connection_options={
        "path": "s3://mon-bucket-glue-tom/output/"
    },
    format="parquet"
)

job.commit()
print("Job terminÃ© avec succÃ¨s!")
```

## ðŸ–¥ï¸ Dashboard

```
1. Uploader le script :
   S3 â†’ mon-bucket-glue-tom â†’ scripts â†’ Upload â†’ transform_customers.py

2. AWS Glue â†’ ETL jobs â†’ Create job

3. Name : customers-transform

4. IAM Role : GlueServiceRole

5. Type : Spark

6. Glue version : Glue 4.0

7. Language : Python 3

8. Script path : s3://mon-bucket-glue-tom/scripts/transform_customers.py

9. Worker type : G.1X
   Number of workers : 2

10. Job parameters (optionnel) :
    --enable-metrics true
    --enable-continuous-cloudwatch-log true

11. Save âœ“
```

## ðŸ’» CLI

```bash
# Uploader le script
aws s3 cp transform_customers.py s3://mon-bucket-glue-tom/scripts/

# CrÃ©er le job
aws glue create-job \
  --name customers-transform \
  --role GlueServiceRole \
  --command '{
    "Name": "glueetl",
    "ScriptLocation": "s3://mon-bucket-glue-tom/scripts/transform_customers.py",
    "PythonVersion": "3"
  }' \
  --default-arguments '{
    "--enable-metrics": "true",
    "--enable-continuous-cloudwatch-log": "true"
  }' \
  --glue-version "4.0" \
  --number-of-workers 2 \
  --worker-type G.1X \
  --region eu-west-3
```

---

# Ã‰tape 7 : ExÃ©cuter le Job

## ðŸ–¥ï¸ Dashboard

```
1. AWS Glue â†’ ETL jobs â†’ customers-transform

2. Run âœ“

3. Suivez l'exÃ©cution dans "Runs"

4. Attendez que le status soit "Succeeded" (5-10 minutes)
```

## ðŸ’» CLI

```bash
# Lancer le job
RUN_ID=$(aws glue start-job-run \
  --job-name customers-transform \
  --query 'JobRunId' \
  --output text \
  --region eu-west-3)

echo "Run ID: $RUN_ID"

# VÃ©rifier le status
aws glue get-job-run \
  --job-name customers-transform \
  --run-id $RUN_ID \
  --query 'JobRun.JobRunState' \
  --region eu-west-3
```

---

# Ã‰tape 8 : VÃ©rifier les rÃ©sultats

## ðŸ–¥ï¸ Dashboard

```
1. S3 â†’ mon-bucket-glue-tom â†’ output/

2. Vous devriez voir des fichiers .parquet

3. VÃ©rifiez le nombre de lignes :
   - Input : 7 lignes
   - Output : 5 lignes (2 supprimÃ©es car NULL)
```

## ðŸ’» CLI

```bash
# Lister les fichiers output
aws s3 ls s3://mon-bucket-glue-tom/output/ --recursive

# TÃ©lÃ©charger et vÃ©rifier (optionnel)
aws s3 cp s3://mon-bucket-glue-tom/output/ ./output/ --recursive
```

---

# Ã‰tape 9 : CrÃ©er un Trigger (Optionnel)

## ðŸ–¥ï¸ Dashboard

```
1. AWS Glue â†’ Triggers â†’ Create trigger

2. Name : daily-transform

3. Trigger type : Schedule

4. Frequency : Daily
   Start hour : 03:00

5. Jobs to trigger : customers-transform

6. Create trigger âœ“

7. Actions â†’ Activate trigger
```

## ðŸ’» CLI

```bash
# CrÃ©er un trigger schedulÃ©
aws glue create-trigger \
  --name daily-transform \
  --type SCHEDULED \
  --schedule "cron(0 3 * * ? *)" \
  --actions '[{"JobName": "customers-transform"}]' \
  --start-on-creation \
  --region eu-west-3
```

---

# ðŸ”§ Troubleshooting

## âŒ Crawler ne trouve pas de donnÃ©es

```
1. VÃ©rifiez le chemin S3 (avec / Ã  la fin)
2. VÃ©rifiez que le fichier CSV existe
3. VÃ©rifiez les permissions du rÃ´le IAM
```

## âŒ Job Ã©choue

```
1. VÃ©rifiez les logs CloudWatch :
   CloudWatch â†’ Log groups â†’ /aws-glue/jobs/error
   
2. VÃ©rifiez le script (syntaxe Python)

3. VÃ©rifiez que le nom de la table correspond
```

## âŒ Pas de fichiers output

```
1. VÃ©rifiez que le job a le status "Succeeded"
2. VÃ©rifiez le chemin S3 de sortie
3. VÃ©rifiez les logs pour voir si des lignes ont Ã©tÃ© traitÃ©es
```

---

# ðŸ§¹ Nettoyage

```bash
# 1. Supprimer le trigger
aws glue delete-trigger --name daily-transform --region eu-west-3

# 2. Supprimer le job
aws glue delete-job --job-name customers-transform --region eu-west-3

# 3. Supprimer le crawler
aws glue delete-crawler --name customers-crawler --region eu-west-3

# 4. Supprimer la table
aws glue delete-table \
  --database-name glue_demo_db \
  --name input \
  --region eu-west-3

# 5. Supprimer la database
aws glue delete-database --name glue_demo_db --region eu-west-3

# 6. Vider et supprimer le bucket S3
aws s3 rm s3://mon-bucket-glue-tom --recursive
aws s3 rb s3://mon-bucket-glue-tom

# 7. Supprimer le rÃ´le IAM
aws iam detach-role-policy --role-name GlueServiceRole \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
aws iam detach-role-policy --role-name GlueServiceRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam delete-role --role-name GlueServiceRole
```

---

## ðŸ“Š RÃ©sumÃ© du Pipeline

| Ã‰tape | Composant | RÃ©sultat |
|-------|-----------|----------|
| 1 | S3 Input | 7 lignes CSV |
| 2 | Crawler | Table crÃ©Ã©e dans Catalog |
| 3 | ETL Job | Nettoyage des NULL |
| 4 | S3 Output | 5 lignes Parquet |

---

## âœ… Checklist Finale

- [ ] Bucket S3 crÃ©Ã© avec structure input/output/scripts
- [ ] Fichier CSV uploadÃ©
- [ ] RÃ´le IAM GlueServiceRole crÃ©Ã©
- [ ] Database Glue crÃ©Ã©e
- [ ] Crawler crÃ©Ã© et exÃ©cutÃ©
- [ ] Table visible dans le Data Catalog
- [ ] Job ETL crÃ©Ã©
- [ ] Job exÃ©cutÃ© avec succÃ¨s
- [ ] Fichiers Parquet gÃ©nÃ©rÃ©s dans output/

---

[â¬…ï¸ Retour : Job5](./Job5_CloudWatch_SNS.md) | [âž¡ï¸ Suite : Job7_Athena_QuickSight.md](./Job7_Athena_QuickSight.md)
